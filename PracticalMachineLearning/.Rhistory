fancyRpartPlot(modFit$finalModel)
summary(modFit$finalModel)
prediction_results<-predict(modFit,newdata=train_testingset)
results_table<-data.frame(prediction_results,train_testingset$classe)
results_table$Correct<-if(results_table$prediction_results==results_table$train_testingset.classe) than 1 else 0
results_table$Correct<-if(results_table$prediction_results==results_table$train_testingset.classe) {"TRUE" than "FALSE" }
results_table$Correct<-if(results_table$prediction_results==results_table$train_testingset.classe) {"TRUE"} else {"FALSE"}
View(results_table)
summary(results_table$Correct)
results_table$Correct<-if(results_table$prediction_results==results_table$train_testingset.classe) than {"TRUE"} else {"FALSE"}
summary(results_table$Correct)
prediction_results<-predict(modFit,newdata=train_testingset)
results_table<-data.frame(prediction_results,train_testingset$classe)
results_table$Correct<-if(results_table$prediction_results==results_table$train_testingset.classe) {1} else {0}
results_table<-data.frame(prediction_results,train_testingset$classe)
results_table$Correct<-if(results_table$prediction_results==results_table$train_testingset.classe) {1} else {0}
results_table$Correct<-identical (results_table$prediction_results,results_table$train_testingset.classe)
View(results_table)
summary(results_table$Correct)
str(results_table)
results_table[1,1]==results_table[1,2]
for (x in 1:1963)
{
if (results_table[x,1]==results_table[x,2])
{results_table[x,3]=="TRUE"} else
{results_table[x,3]=="FALSE"}
}
View(results_table)
View(results_table)
View(results_table)
remove(results_table)
prediction_results<-predict(modFit,newdata=train_testingset)
results_table<-data.frame(prediction_results,train_testingset$classe)
View(results_table)
results_table$Correct<-"Nil"
View(results_table)
for (x in 1:5)
{
if (results_table[x,1]==results_table[x,2])
{results_table[x,3]=="TRUE"} else
{results_table[x,3]=="FALSE"}
}
View(results_table)
for (x in 1:5)
{
if (results_table[x,1]==results_table[x,2])
{results_table[x,3]<-"TRUE"} else
{results_table[x,3]<-"FALSE"}
}
View(results_table)
for (x in 1:nrows(results_table))
{
if (results_table[x,1]==results_table[x,2])
{results_table[x,3]<-"TRUE"} else
{results_table[x,3]<-"FALSE"}
}
for (x in 1:nrow(results_table))
{
if (results_table[x,1]==results_table[x,2])
{results_table[x,3]<-"TRUE"} else
{results_table[x,3]<-"FALSE"}
}
summary(results_table$Correct)}
summary(results_table$Correct)
View(results_table)
str(results_table)
summary(results_table$Correct)
summary(as.factor(results_table$Correct))
results_summary<-summary(as.factor(results_table$Correct))
results_summary(1)
results_summary[1]
acccuracy<-results_summary[2]/nrow(results_summary)
accurancy
accuracy<-results_summary[2]/nrow(results_summary)
accuracy
results_summary[2]
nrow(results_summary)
accuracy<-results_summary[2]/nrow(results_summartable)
accuracy
results_summary<-summary(as.factor(results_table$Correct))
accuracy<-results_summary[2]/nrow(results_table)
accuracy
results_summary<-summary(as.factor(results_table$Correct))
accuracy<-results_summary[2]/nrow(results_table)
accuracy
---
title: "Practical Machine Learning"
author: "Yuani"
date: "19 November 2015"
output: html_document
---
##1. Download Data
The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har.
When importing data, all empty cells were replaced in "NA".str
```{r}
setwd("C:\\Users\\ychen06\\Documents\\IDA MOOC\\Git\\PracticalMachineLearning")
data_training<-read.csv("pml-training.csv",na.strings=c(""," ","NA"))
data_testing<-read.csv("pml-testing.csv",na.strings=c(""," ","NA"))
```
##Clean Dataset
There are 160 different variables in the datasets provided. However, not all variables can be used in developing  our predictions as the entire variable consist of null values.
We first clean the data set to select only variables with values that can be used for analysis.Columns with complete "NA" or blanks are removed.
We also generated a second table consisting of all the predictor variables and the final desired predicted value "classe". These variables were selected based on the goal of using data from accelerometers on the bell, forearm, arm and dumbbell.
```{r}
data_training_cleaned<-data_training[ , colSums(is.na(data_training))== 0 ]
data_predictor<- data_training_cleaned[,grepl("belt|forearm|arm|dumbbell|classe",names(data_training_cleaned))]
data_testing_cleaned<-data_testing[ , colSums(is.na(data_testing))== 0 ]
data_testing_cleaned<- data_testing_cleaned[,grepl("belt|forearm|arm|dumbbell|classe",names(data_testing_cleaned))]
```
This results in 53 variables including "classe" which is our predcted outcome.
```{r}
names(data_predictor)
```
The predicted outcomes in our training set is distributed as follow:
```{r}
summary(data_predictor$classe)
```
## Prediction Modeling
To create a check for our prediction model, we split the training set into 10 parts, 9 for training and 1 for cross validation. We first train a model using 9 folds, and test it on the 10th fold to check for accuracy.
```{r}
library(caret)
library(rpart)
folds<-createFolds(y=data_predictor$classe,k=10,list=FALSE)
train_trainingset<-data_predictor[folds != 10,]
train_testingset<-data_predictor[folds == 10,]
set.seed(123)
modFit<-train(classe ~ . ,method="rpart",data=train_testingset)
print(modFit$finalModel)
library(rattle)
rattle()
fancyRpartPlot(modFit$finalModel)
prediction_results<-predict(modFit,newdata=train_testingset)
results_table<-data.frame(prediction_results,train_testingset$classe)
results_table$Correct<-"Nil"
for (x in 1:nrow(results_table))
{
if (results_table[x,1]==results_table[x,2])
{results_table[x,3]<-"TRUE"} else
{results_table[x,3]<-"FALSE"}
}
results_summary<-summary(as.factor(results_table$Correct))
accuracy<-results_summary[2]/nrow(results_table)
accuracy
```
train_testingset<-data_predictor[folds == 12,]
train_testingset<-data_predictor[folds == 15,]
##1. Download Data
The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har.
When importing data, all empty cells were replaced in "NA".str
```{r}
setwd("C:\\Users\\ychen06\\Documents\\IDA MOOC\\Git\\PracticalMachineLearning")
data_training<-read.csv("pml-training.csv",na.strings=c(""," ","NA"))
data_testing<-read.csv("pml-testing.csv",na.strings=c(""," ","NA"))
```
##Clean Dataset
There are 160 different variables in the datasets provided. However, not all variables can be used in developing  our predictions as the entire variable consist of null values.
We first clean the data set to select only variables with values that can be used for analysis.Columns with complete "NA" or blanks are removed.
We also generated a second table consisting of all the predictor variables and the final desired predicted value "classe". These variables were selected based on the goal of using data from accelerometers on the bell, forearm, arm and dumbbell.
```{r}
data_training_cleaned<-data_training[ , colSums(is.na(data_training))== 0 ]
data_predictor<- data_training_cleaned[,grepl("belt|forearm|arm|dumbbell|classe",names(data_training_cleaned))]
data_testing_cleaned<-data_testing[ , colSums(is.na(data_testing))== 0 ]
data_testing_cleaned<- data_testing_cleaned[,grepl("belt|forearm|arm|dumbbell|classe",names(data_testing_cleaned))]
```
This results in 53 variables including "classe" which is our predcted outcome.
```{r}
names(data_predictor)
```
The predicted outcomes in our training set is distributed as follow:
```{r}
summary(data_predictor$classe)
```
## Prediction Modeling
To create a check for our prediction model, we split the training set into 10 parts, 9 for training and 1 for cross validation. We first train a model using 9 folds, and test it on the 10th fold to check for accuracy.
```{r}
library(caret)
library(rpart)
folds<-createFolds(y=data_predictor$classe,k=15,list=FALSE)
train_trainingset<-data_predictor[folds != 15,]
train_testingset<-data_predictor[folds == 15,]
set.seed(123)
modFit<-train(classe ~ . ,method="rpart",data=train_trainingset)
print(modFit$finalModel)
library(rattle)
rattle()
fancyRpartPlot(modFit$finalModel)
prediction_results<-predict(modFit,newdata=train_testingset)
confusionMatrix(prediction_results, train_testingset$classe)
```{r}
library(caret)
library(rpart)
inTrain <- createDataPartition(y=data_training$classe, p=0.7, list=FALSE)
train_trainingset <- data_training[inTrain,]
train_testingset<- data_training[-inTrain,]
set.seed(1212)
#Decision Tree Method
modFit<-train(classe ~ . ,method="rpart",data=train_trainingset)
print(modFit$finalModel)
library(rattle)
rattle()
fancyRpartPlot(modFit$finalModel)
prediction_results_tree<-predict(modFit,newdata=train_testingset)
confusionMatrix(prediction_results_tree, train_testingset$classe)
#Random Forest Method
modFit2<-train(classe ~ . ,method="rf",data=train_trainingset,prox=TRUE)
print(modFit2$finalModel)
prediction_results_rf<-predict(modFit2,newdata=train_testingset)
confusionMatrix(prediction_results_rf, train_testingset$classe)
modFit2<-train(classe ~ . ,method="rf",data=train_trainingset,prox=TRUE)
str(train_trainingset$classe)
modFit2<-train(classe ~ . ,method="rf",data=train_trainingset)
str(train_trainingset)
library(caret)
library(rpart)
inTrain <- createDataPartition(y=data_training_cleaned$classe, p=0.7, list=FALSE)
train_trainingset <- data_training_cleaned[inTrain,]
train_testingset<- data_training_cleaned[-inTrain,]
set.seed(1212)
#Decision Tree Method
modFit<-train(classe ~ . ,method="rpart",data=train_trainingset)
print(modFit$finalModel)
library(rattle)
rattle()
fancyRpartPlot(modFit$finalModel)
prediction_results_tree<-predict(modFit,newdata=train_testingset)
confusionMatrix(prediction_results_tree, train_testingset$classe)
#Random Forest Method
modFit2<-train(classe ~ . ,method="rf",data=train_trainingset,prox=TRUE)
install.packages(randomForest)
install.packages('randomForest')
install.packages('randomForest')
---
title: "Practical Machine Learning"
author: "Yuani"
date: "19 November 2015"
output: html_document
---
##1. Download Data
The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har.
When importing data, all empty cells were replaced in "NA".str
```{r}
setwd("C:\\Users\\ychen06\\Documents\\IDA MOOC\\Git\\PracticalMachineLearning")
data_training<-read.csv("pml-training.csv",na.strings=c(""," ","NA"))
data_testing<-read.csv("pml-testing.csv",na.strings=c(""," ","NA"))
```
##Clean Dataset
There are 160 different variables in the datasets provided. However, not all variables can be used in developing  our predictions as the entire variable consist of null values.
We first clean the data set to select only variables with values that can be used for analysis.Columns with complete "NA" or blanks are removed.
We also generated a second table consisting of all the predictor variables and the final desired predicted value "classe". These variables were selected based on the goal of using data from accelerometers on the bell, forearm, arm and dumbbell.
```{r}
data_training_cleaned<-data_training[ , colSums(is.na(data_training))== 0 ]
data_predictor<- data_training_cleaned[,grepl("belt|forearm|arm|dumbbell|classe",names(data_training_cleaned))]
data_testing_cleaned<-data_testing[ , colSums(is.na(data_testing))== 0 ]
data_testing_cleaned<- data_testing_cleaned[,grepl("belt|forearm|arm|dumbbell|classe",names(data_testing_cleaned))]
```
This results in 53 variables including "classe" which is our predcted outcome.
```{r}
names(data_predictor)
```
The predicted outcomes in our training set is distributed as follow:
```{r}
summary(data_predictor$classe)
```
## Prediction Modeling
To create a check for our prediction model, we split the training set into 10 parts, 9 for training and 1 for cross validation. We first train a model using 9 folds, and test it on the 10th fold to check for accuracy.
```{r}
library(caret)
library(rpart)
inTrain <- createDataPartition(y=data_training_cleaned$classe, p=0.7, list=FALSE)
train_trainingset <- data_training_cleaned[inTrain,]
train_testingset<- data_training_cleaned[-inTrain,]
set.seed(1212)
#Decision Tree Method
modFit<-train(classe ~ . ,method="rpart",data=train_trainingset)
print(modFit$finalModel)
library(rattle)
rattle()
fancyRpartPlot(modFit$finalModel)
prediction_results_tree<-predict(modFit,newdata=train_testingset)
confusionMatrix(prediction_results_tree, train_testingset$classe)
#Bagging Method
modFit2<-train(classe ~ . ,method="treebag",data=train_trainingset,prox=TRUE)
install.packages('ipred')
modFit2<-train(classe ~ . ,method="treebag",data=train_trainingset,prox=TRUE)
print(modFit2$finalModel)
prediction_results_rf<-predict(modFit2,newdata=train_testingset)
confusionMatrix(prediction_results_rf, train_testingset$classe)
library(ipred)
modFit2<-train(classe ~ . ,method="treebag",data=train_trainingset,prox=TRUE)
print(modFit2$finalModel)
prediction_results_rf<-predict(modFit2,newdata=train_testingset)
confusionMatrix(prediction_results_rf, train_testingset$classe)
library('ipred')
install.packages('ipred')
library(ipred)
#Bagging Method
library(ipred)
modFit2<-train(classe ~ . ,method="bagEarth",data=train_trainingset,prox=TRUE)
library(earth)
library(plotmo)
library(plotrix)
library(TeachingDemos)
modFit2<-train(classe ~ . ,method="bagEarth",data=train_trainingset,prox=TRUE)
print(modFit2$finalModel)
prediction_results_rf<-predict(modFit2,newdata=train_testingset)
confusionMatrix(prediction_results_rf, train_testingset$classe)
title: "Practical Machine Learning"
author: "Yuani"
date: "19 November 2015"
output: html_document
---
##1. Download Data
The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har.
When importing data, all empty cells were replaced in "NA".str
```{r}
setwd("C:\\Users\\ychen06\\Documents\\IDA MOOC\\Git\\PracticalMachineLearning")
data_training<-read.csv("pml-training.csv",na.strings=c(""," ","NA"))
data_testing<-read.csv("pml-testing.csv",na.strings=c(""," ","NA"))
```
##Clean Dataset
There are 160 different variables in the datasets provided. However, not all variables can be used in developing  our predictions as the entire variable consist of null values.
We first clean the data set to select only variables with values that can be used for analysis.Columns with complete "NA" or blanks are removed.
We also generated a second table consisting of all the predictor variables and the final desired predicted value "classe". These variables were selected based on the goal of using data from accelerometers on the bell, forearm, arm and dumbbell.
```{r}
data_training_cleaned<-data_training[ , colSums(is.na(data_training))== 0 ]
data_predictor<- data_training_cleaned[,grepl("belt|forearm|arm|dumbbell|classe",names(data_training_cleaned))]
data_testing_cleaned<-data_testing[ , colSums(is.na(data_testing))== 0 ]
data_testing_cleaned<- data_testing_cleaned[,grepl("belt|forearm|arm|dumbbell|classe",names(data_testing_cleaned))]
```
This results in 53 variables including "classe" which is our predcted outcome.
```{r}
names(data_predictor)
```
The predicted outcomes in our training set is distributed as follow:
```{r}
summary(data_predictor$classe)
```
## Prediction Modeling
To create a check for our prediction model, we split the training set into 10 parts, 9 for training and 1 for cross validation. We first train a model using 9 folds, and test it on the 10th fold to check for accuracy.
```{r}
library(caret)
library(rpart)
inTrain <- createDataPartition(y=data_training_cleaned$classe, p=0.7, list=FALSE)
train_trainingset <- data_training_cleaned[inTrain,]
train_testingset<- data_training_cleaned[-inTrain,]
set.seed(1212)
#Decision Tree Method
modFit<-train(classe ~ . ,method="rpart",data=train_trainingset)
print(modFit$finalModel)
library(rattle)
rattle()
fancyRpartPlot(modFit$finalModel)
prediction_results_tree<-predict(modFit,newdata=train_testingset)
confusionMatrix(prediction_results_tree, train_testingset$classe)
#Bagging Method
library(earth)
library(plotmo)
library(plotrix)
library(TeachingDemos)
modFit2<-train(classe ~ . ,method="bagEarth",data=train_trainingset,prox=TRUE)
print(modFit2$finalModel)
prediction_results_rf<-predict(modFit2,newdata=train_testingset)
confusionMatrix(prediction_results_rf, train_testingset$classe)
inTrain <- createDataPartition(y=data_predictor$classe, p=0.7, list=FALSE)
train_trainingset <- data_predictor[inTrain,]
train_testingset<- data_predictor[-inTrain,]
library(earth)
library(plotmo)
library(plotrix)
library(TeachingDemos)
modFit2<-train(classe ~ . ,method="bagEarth",data=train_trainingset,prox=TRUE)
print(modFit2$finalModel)
prediction_results_rf<-predict(modFit2,newdata=train_testingset)
confusionMatrix(prediction_results_rf, train_testingset$classe)
View(train_trainingset)
confusionMatrix(prediction_results_rf, train_testingset$classe)
summary(train_trainingset)
modFit2<-train(classe ~ . ,method="bagFDA",data=train_trainingset,prox=TRUE)
modFit2<-train(classe ~ . ,method="bagFDA",data=train_trainingset)
modFit2<-train(classe ~ . ,method="treebag",data=train_trainingset)
print(modFit2$finalModel)
prediction_results_rf<-predict(modFit2,newdata=train_testingset)
confusionMatrix(prediction_results_rf, train_testingset$classe)
modFit2<-train(classe ~ . ,method="bagEarth",data=train_trainingset)
#Random Forest Method
library(randomForest)
modFit2<-randomForest(classe~., data=train_trainingset, ntree = 500)
print(modFit2$finalModel)
prediction_results_rf<-predict(modFit2,newdata=train_testingset)
confusionMatrix(prediction_results_rf, train_testingset$classe)
cm1<-confusionMatrix(prediction_results_tree, train_testingset$classe)
cm2<-confusionMatrix(prediction_results_rf, train_testingset$classe)
par(mfrow=c(1,2))
plot(cm1$byClass, main="classification tree", xlim=c(0.4, 1.005), ylim=c(0.7,1))
text(cm1$byClass[,1]+0.04, cm0$byClass[,2], labels=LETTERS[1:5], cex= 0.7)
plot(cm2$byClass, main="random forest", xlim=c(0.96, 1.005))
text(cm2$byClass[,1]+0.003, cm1$byClass[,2], labels=LETTERS[1:5], cex= 0.7)
par(mfrow=c(1,2))
plot(cm1$byClass, main="classification tree", xlim=c(0.1, 1.1), ylim=c(0.1,1))
text(cm1$byClass[,1]+0.04, cm0$byClass[,2], labels=LETTERS[1:5], cex= 0.7)
plot(cm2$byClass, main="random forest", xlim=c(0.96, 1.005))
text(cm2$byClass[,1]+0.003, cm1$byClass[,2], labels=LETTERS[1:5], cex= 0.7)
par(mfrow=c(1,2))
plot(cm1$byClass, main="classification tree")
text(cm1$byClass[,1]+0.04, cm0$byClass[,2], labels=LETTERS[1:5], cex= 0.7)
plot(cm2$byClass, main="random forest")
text(cm2$byClass[,1]+0.003, cm1$byClass[,2], labels=LETTERS[1:5], cex= 0.7)
par(mfrow=c(1,2))
plot(cm1$byClass, main="classification tree")
text(cm1$byClass[,1]+0.04, cm0$byClass[,2], labels=LETTERS[1:5], cex= 0.7)
plot(cm2$byClass, main="random forest",xlim=c(0.95,1.05))
text(cm2$byClass[,1]+0.003, cm1$byClass[,2], labels=LETTERS[1:5], cex= 0.7)
plot(cm2$byClass, main="random forest",xlim=c(0.95,1.01),ylim=c(0.95,1.01))
text(cm2$byClass[,1]+0.003, cm1$byClass[,2], labels=LETTERS[1:5], cex= 0.7)
par(mfrow=c(1,2))
plot(cm1$byClass, main="classification tree")
text(cm1$byClass[,1]+0.04, cm0$byClass[,2], labels=LETTERS[1:5], cex= 0.7)
plot(cm2$byClass, main="random forest",xlim=c(0.95,1.01),ylim=c(0.95,1.01))
text(cm2$byClass[,1]+0.003, cm1$byClass[,2], labels=LETTERS[1:5], cex= 0.7)
prediction_results_final<-predict(modFit2,newdata=data_testing_cleaned)
final_table<-data_testing_clean
final_table$Prediction<-prediction_results_final
prediction_results_final<-predict(modFit2,newdata=data_testing_cleaned)
final_table<-data_testing_cleaned
final_table$Prediction<-prediction_results_final
View(final_table)
library(caret)
library(rpart)
library(ggplot2)
inTrain <- createDataPartition(y=data_predictor$classe, p=0.7, list=FALSE)
train_trainingset <- data_predictor[inTrain,]
train_testingset<- data_predictor[-inTrain,]
set.seed(1212)
#Decision Tree Method
modFit<-train(classe ~ . ,method="rpart",data=train_trainingset)
print(modFit$finalModel)
library(rattle)
rattle()
fancyRpartPlot(modFit$finalModel)
prediction_results_tree<-predict(modFit,newdata=train_testingset)
cm1<-confusionMatrix(prediction_results_tree, train_testingset$classe)
#Random Forest Method
library(randomForest)
modFit2<-randomForest(classe~., data=train_trainingset, ntree = 500)
print(modFit2$finalModel)
prediction_results_rf<-predict(modFit2,newdata=train_testingset)
cm2<-confusionMatrix(prediction_results_rf, train_testingset$classe)
library(caret)
library(rpart)
library(ggplot2)
inTrain <- createDataPartition(y=data_predictor$classe, p=0.7, list=FALSE)
train_trainingset <- data_predictor[inTrain,]
train_testingset<- data_predictor[-inTrain,]
set.seed(1212)
#Decision Tree Method
modFit<-train(classe ~ . ,method="rpart",data=train_trainingset)
print(modFit$finalModel)
library(rattle)
rattle()
fancyRpartPlot(modFit$finalModel)
prediction_results_tree<-predict(modFit,newdata=train_testingset)
cm1<-confusionMatrix(prediction_results_tree, train_testingset$classe)
#Random Forest Method
library(randomForest)
modFit2<-randomForest(classe~., data=train_trainingset, ntree = 500)
print(modFit2$finalModel)
prediction_results_rf<-predict(modFit2,newdata=train_testingset)
cm2<-confusionMatrix(prediction_results_rf, train_testingset$classe)
---
title: "Practical Machine Learning"
author: "Yuani"
date: "19 November 2015"
output: html_document
---
##1. Download Data
The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har.
When importing data, all empty cells were replaced in "NA".str
```{r}
setwd("C:\\Users\\ychen06\\Documents\\IDA MOOC\\Git\\PracticalMachineLearning")
data_training<-read.csv("pml-training.csv",na.strings=c(""," ","NA"))
data_testing<-read.csv("pml-testing.csv",na.strings=c(""," ","NA"))
```
##Clean Dataset
There are 160 different variables in the datasets provided. However, not all variables can be used in developing  our predictions as the entire variable consist of null values.
We first clean the data set to select only variables with values that can be used for analysis.Columns with complete "NA" or blanks are removed.
We also generated a second table consisting of all the predictor variables and the final desired predicted value "classe". These variables were selected based on the goal of using data from accelerometers on the bell, forearm, arm and dumbbell.
```{r}
data_training_cleaned<-data_training[ , colSums(is.na(data_training))== 0 ]
data_predictor<- data_training_cleaned[,grepl("belt|forearm|arm|dumbbell|classe",names(data_training_cleaned))]
data_testing_cleaned<-data_testing[ , colSums(is.na(data_testing))== 0 ]
data_testing_cleaned<- data_testing_cleaned[,grepl("belt|forearm|arm|dumbbell|classe",names(data_testing_cleaned))]
```
This results in 53 variables including "classe" which is our predcted outcome.
```{r}
names(data_predictor)
```
The predicted outcomes in our training set is distributed as follow:
```{r}
summary(data_predictor$classe)
```
## Prediction Modeling
To create a check for our prediction model, we split the training set into 10 parts, 9 for training and 1 for cross validation. We first train a model using 9 folds, and test it on the 10th fold to check for accuracy.
```{r}
library(caret)
library(rpart)
library(ggplot2)
inTrain <- createDataPartition(y=data_predictor$classe, p=0.7, list=FALSE)
train_trainingset <- data_predictor[inTrain,]
train_testingset<- data_predictor[-inTrain,]
set.seed(1212)
#Decision Tree Method
modFit<-train(classe ~ . ,method="rpart",data=train_trainingset)
